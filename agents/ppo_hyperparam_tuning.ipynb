{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48eaab64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "\n",
    "sys.path.append(os.path.abspath(\"..\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c6c50d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Stable Baselines 3 PPO\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "\n",
    "# Your custom environment\n",
    "from env.stardew_mine_env import StardewMineEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfddece5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ppo(env, learning_rate=3e-4, n_steps=2048, batch_size=64, gamma=0.99, total_timesteps=10000, seed=None):\n",
    "    env = DummyVecEnv([lambda: env])\n",
    "    \n",
    "    model = PPO(\n",
    "        \"MultiInputPolicy\",\n",
    "        env,\n",
    "        learning_rate=learning_rate,\n",
    "        n_steps=n_steps,\n",
    "        batch_size=batch_size,\n",
    "        gamma=gamma,\n",
    "        verbose=0,\n",
    "        seed=seed\n",
    "    )\n",
    "    \n",
    "    model.learn(total_timesteps=total_timesteps)\n",
    "    \n",
    "    mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=10, deterministic=True)\n",
    "    \n",
    "    return mean_reward, std_reward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6db976e",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparams = {\n",
    "    \"learning_rate\": [1e-4, 3e-4, 1e-3],\n",
    "    \"n_steps\": [512, 2048, 4096],\n",
    "    \"batch_size\": [32, 64, 128],\n",
    "    \"gamma\": [0.95, 0.99, 0.999]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb5b7c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "for lr in hyperparams['learning_rate']:\n",
    "    for n in hyperparams['n_steps']:\n",
    "        for batch in hyperparams['batch_size']:\n",
    "            for g in hyperparams['gamma']:\n",
    "                env = StardewMineEnv()\n",
    "                mean_reward, std_reward = train_ppo(env, learning_rate=lr, n_steps=n, batch_size=batch, gamma=g)\n",
    "                \n",
    "                results.append({\n",
    "                    \"learning_rate\": lr,\n",
    "                    \"n_steps\": n,\n",
    "                    \"batch_size\": batch,\n",
    "                    \"gamma\": g,\n",
    "                    \"mean_reward\": mean_reward,\n",
    "                    \"std_reward\": std_reward\n",
    "                })\n",
    "                print(f\"Done: lr={lr}, n_steps={n}, batch={batch}, gamma={g}, reward={mean_reward:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65123863",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(results)\n",
    "df.to_csv(\"ppo_hyperparam_results.csv\", index=False)\n",
    "\n",
    "# Example plots\n",
    "import seaborn as sns\n",
    "\n",
    "sns.lineplot(data=df, x=\"learning_rate\", y=\"mean_reward\", marker=\"o\")\n",
    "plt.title(\"Mean Reward vs Learning Rate\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hw2-py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
