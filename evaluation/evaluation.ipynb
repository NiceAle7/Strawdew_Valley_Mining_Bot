{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "11bc849a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "\n",
    "from env.stardew_mine_env import StardewMineEnv\n",
    "\n",
    "from evaluation.visualization_tools import (\n",
    "    plot_reward_curve,\n",
    "    plot_training_curves,\n",
    "    plot_heatmap,\n",
    "    plot_bar\n",
    ")\n",
    "\n",
    "from stable_baselines3 import PPO\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "871541f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "from stable_baselines3 import PPO\n",
    "model = PPO.load(\"../models/ppo_miningbot.zip\")\n",
    "print(\"Model loaded successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ca84be98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_agent(agent, env, episodes=20, is_ppo=True):\n",
    "    rewards = []\n",
    "    ores = []\n",
    "    floors = []\n",
    "    energies = []\n",
    "    heatmaps = []\n",
    "\n",
    "    # infer grid size robustly (from env.grid, observation_space, or SIZE)\n",
    "    if hasattr(env, \"grid\"):\n",
    "        h, w = env.grid.shape\n",
    "    else:\n",
    "        os = getattr(env, 'observation_space', None)\n",
    "        if os is not None and hasattr(os, 'spaces') and 'local_view' in os.spaces:\n",
    "            h, w = os.spaces['local_view'].shape[:2]\n",
    "        else:\n",
    "            h = w = getattr(env, 'SIZE', getattr(env, 'size', 10))\n",
    "\n",
    "    def _normalize_obs(obs, obs_space):\n",
    "        # Ensure each dict observation field is a numpy array with the\n",
    "        # shape expected by the observation space (no 0-d scalars).\n",
    "        if not isinstance(obs, dict) or obs_space is None:\n",
    "            return obs\n",
    "\n",
    "        norm = {}\n",
    "        spaces = getattr(obs_space, 'spaces', {})\n",
    "        for k, v in obs.items():\n",
    "            space = spaces.get(k)\n",
    "            arr = np.asarray(v)\n",
    "            # cast dtype when possible\n",
    "            try:\n",
    "                if space is not None and hasattr(space, 'dtype'):\n",
    "                    arr = arr.astype(space.dtype)\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "            # expand 0-d scalars into shape expected by the Box\n",
    "            try:\n",
    "                if arr.shape == () and space is not None:\n",
    "                    arr = arr.reshape(space.shape)\n",
    "                elif space is not None and arr.shape != space.shape and arr.size == np.prod(space.shape):\n",
    "                    arr = arr.reshape(space.shape)\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "            norm[k] = arr\n",
    "\n",
    "        return norm\n",
    "\n",
    "    for ep in range(episodes):\n",
    "        # reset may return (obs, info) (Gymnasium) or obs (older Gym)\n",
    "        reset_result = env.reset()\n",
    "        if isinstance(reset_result, tuple):\n",
    "            # common Gymnasium signature: (obs, info)\n",
    "            obs = reset_result[0]\n",
    "            info = reset_result[1] if len(reset_result) > 1 else {}\n",
    "        else:\n",
    "            obs = reset_result\n",
    "            info = {}\n",
    "\n",
    "        total_reward = 0\n",
    "        visit_count = np.zeros((h, w), dtype=int)\n",
    "\n",
    "        # track initial ore count so we can report collected ore\n",
    "        try:\n",
    "            initial_ore = int(np.sum(env.grid == getattr(env, 'ORE', 4)))\n",
    "        except Exception:\n",
    "            initial_ore = 0\n",
    "\n",
    "        last_pos = info.get('agent_pos', None)\n",
    "\n",
    "        done = False\n",
    "        while not done:\n",
    "\n",
    "            # Determine current player location (prefer info, then obs)\n",
    "            if last_pos is not None:\n",
    "                x, y = last_pos\n",
    "            else:\n",
    "                x = y = None\n",
    "                if isinstance(obs, dict):\n",
    "                    al = obs.get('agent_location', None)\n",
    "                    if al is not None:\n",
    "                        try:\n",
    "                            x = int(round(float(al[0])))\n",
    "                            y = int(round(float(al[1])))\n",
    "                        except Exception:\n",
    "                            x = y = None\n",
    "                elif isinstance(obs, np.ndarray):\n",
    "                    try:\n",
    "                        y, x = np.unravel_index(obs.argmax(), obs.shape)\n",
    "                        x = int(x); y = int(y)\n",
    "                    except Exception:\n",
    "                        x = y = None\n",
    "\n",
    "            if x is not None and 0 <= x < w and 0 <= y < h:\n",
    "                visit_count[y][x] += 1\n",
    "\n",
    "            # Choose action. Normalize observations to match SB3 expectations\n",
    "            obs_for_agent = _normalize_obs(obs, getattr(env, 'observation_space', None))\n",
    "            if is_ppo:\n",
    "                action, _ = agent.predict(obs_for_agent)\n",
    "            else:\n",
    "                # support legacy agents that expect the raw obs\n",
    "                action = agent.act(obs_for_agent) if hasattr(agent, 'act') else agent.predict(obs_for_agent)[0]\n",
    "\n",
    "            # Step env (support Gymnasium and Gym signatures)\n",
    "            step_result = env.step(action)\n",
    "            if isinstance(step_result, tuple):\n",
    "                if len(step_result) == 5:\n",
    "                    obs, reward, terminated, truncated, info = step_result\n",
    "                    done = bool(terminated or truncated)\n",
    "                elif len(step_result) == 4:\n",
    "                    obs, reward, done, info = step_result\n",
    "                else:\n",
    "                    # unexpected tuple length; best-effort unpack\n",
    "                    obs = step_result[0]\n",
    "                    reward = float(step_result[1]) if len(step_result) > 1 else 0.0\n",
    "                    done = False\n",
    "                    info = step_result[-1] if len(step_result) > 2 else {}\n",
    "            else:\n",
    "                obs = step_result\n",
    "                reward = 0.0\n",
    "                done = False\n",
    "                info = {}\n",
    "\n",
    "            total_reward += float(reward)\n",
    "\n",
    "            # Update position (preferred source)\n",
    "            last_pos = info.get('agent_pos', None)\n",
    "\n",
    "        # Episode finished: record stats\n",
    "        rewards.append(total_reward)\n",
    "        try:\n",
    "            remaining_ore = int(np.sum(env.grid == getattr(env, 'ORE', 4)))\n",
    "            ores.append(initial_ore - remaining_ore)\n",
    "        except Exception:\n",
    "            ores.append(info.get('ore_collected', 0))\n",
    "        floors.append(getattr(env, 'floor', info.get('floor', 0)))\n",
    "        energies.append(getattr(env, 'energy', info.get('energy', 0)))\n",
    "        heatmaps.append(visit_count)\n",
    "\n",
    "    return rewards, ores, floors, energies, heatmaps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ecfe5afd",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error: Unexpected observation shape () for Box environment, please use (1,) or (n_env, 1) for the observation shape.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m env \u001b[38;5;241m=\u001b[39m StardewMineEnv(size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m ppo_rewards, ppo_ores, ppo_floors, ppo_energy, ppo_heatmaps \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_agent\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepisodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_ppo\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m      5\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[31], line 100\u001b[0m, in \u001b[0;36mevaluate_agent\u001b[0;34m(agent, env, episodes, is_ppo)\u001b[0m\n\u001b[1;32m     98\u001b[0m obs_for_agent \u001b[38;5;241m=\u001b[39m _normalize_obs(obs, \u001b[38;5;28mgetattr\u001b[39m(env, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mobservation_space\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_ppo:\n\u001b[0;32m--> 100\u001b[0m     action, _ \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs_for_agent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    102\u001b[0m     \u001b[38;5;66;03m# support legacy agents that expect the raw obs\u001b[39;00m\n\u001b[1;32m    103\u001b[0m     action \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39mact(obs_for_agent) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(agent, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mact\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m agent\u001b[38;5;241m.\u001b[39mpredict(obs_for_agent)[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m/opt/anaconda3/envs/hw2-py310/lib/python3.10/site-packages/stable_baselines3/common/base_class.py:557\u001b[0m, in \u001b[0;36mBaseAlgorithm.predict\u001b[0;34m(self, observation, state, episode_start, deterministic)\u001b[0m\n\u001b[1;32m    537\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mpredict\u001b[39m(\n\u001b[1;32m    538\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    539\u001b[0m     observation: Union[np\u001b[38;5;241m.\u001b[39mndarray, \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, np\u001b[38;5;241m.\u001b[39mndarray]],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    542\u001b[0m     deterministic: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    543\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[np\u001b[38;5;241m.\u001b[39mndarray, Optional[\u001b[38;5;28mtuple\u001b[39m[np\u001b[38;5;241m.\u001b[39mndarray, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m]]]:\n\u001b[1;32m    544\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    545\u001b[0m \u001b[38;5;124;03m    Get the policy action from an observation (and optional hidden state).\u001b[39;00m\n\u001b[1;32m    546\u001b[0m \u001b[38;5;124;03m    Includes sugar-coating to handle different observations (e.g. normalizing images).\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    555\u001b[0m \u001b[38;5;124;03m        (used in recurrent policies)\u001b[39;00m\n\u001b[1;32m    556\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 557\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpolicy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobservation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepisode_start\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdeterministic\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/hw2-py310/lib/python3.10/site-packages/stable_baselines3/common/policies.py:365\u001b[0m, in \u001b[0;36mBasePolicy.predict\u001b[0;34m(self, observation, state, episode_start, deterministic)\u001b[0m\n\u001b[1;32m    356\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(observation, \u001b[38;5;28mtuple\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(observation) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(observation[\u001b[38;5;241m1\u001b[39m], \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m    357\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    358\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou have passed a tuple to the predict() function instead of a Numpy array or a Dict. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    359\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are probably mixing Gym API with SB3 VecEnv API: `obs, info = env.reset()` (Gym) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    362\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mand documentation for more information: https://stable-baselines3.readthedocs.io/en/master/guide/vec_envs.html#vecenv-api-vs-gym-api\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    363\u001b[0m     )\n\u001b[0;32m--> 365\u001b[0m obs_tensor, vectorized_env \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobs_to_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobservation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    367\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m th\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m    368\u001b[0m     actions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_predict(obs_tensor, deterministic\u001b[38;5;241m=\u001b[39mdeterministic)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/hw2-py310/lib/python3.10/site-packages/stable_baselines3/common/policies.py:258\u001b[0m, in \u001b[0;36mBaseModel.obs_to_tensor\u001b[0;34m(self, observation)\u001b[0m\n\u001b[1;32m    256\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    257\u001b[0m     obs_ \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(obs)\n\u001b[0;32m--> 258\u001b[0m vectorized_env \u001b[38;5;241m=\u001b[39m vectorized_env \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mis_vectorized_observation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobs_space\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[38;5;66;03m# Add batch dimension if needed\u001b[39;00m\n\u001b[1;32m    260\u001b[0m observation[key] \u001b[38;5;241m=\u001b[39m obs_\u001b[38;5;241m.\u001b[39mreshape((\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobservation_space[key]\u001b[38;5;241m.\u001b[39mshape))  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/hw2-py310/lib/python3.10/site-packages/stable_baselines3/common/utils.py:489\u001b[0m, in \u001b[0;36mis_vectorized_observation\u001b[0;34m(observation, observation_space)\u001b[0m\n\u001b[1;32m    487\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m space_type, is_vec_obs_func \u001b[38;5;129;01min\u001b[39;00m is_vec_obs_func_dict\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    488\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(observation_space, space_type):\n\u001b[0;32m--> 489\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mis_vec_obs_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobservation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobservation_space\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[operator]\u001b[39;00m\n\u001b[1;32m    490\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    491\u001b[0m     \u001b[38;5;66;03m# for-else happens if no break is called\u001b[39;00m\n\u001b[1;32m    492\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError: Cannot determine if the observation is vectorized with the space type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mobservation_space\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/hw2-py310/lib/python3.10/site-packages/stable_baselines3/common/utils.py:356\u001b[0m, in \u001b[0;36mis_vectorized_box_observation\u001b[0;34m(observation, observation_space)\u001b[0m\n\u001b[1;32m    354\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    355\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 356\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    357\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError: Unexpected observation shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mobservation\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    358\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBox environment, please use \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mobservation_space\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    359\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mor (n_env, \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m) for the observation shape.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mmap\u001b[39m(\u001b[38;5;28mstr\u001b[39m, observation_space\u001b[38;5;241m.\u001b[39mshape)))\n\u001b[1;32m    360\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Error: Unexpected observation shape () for Box environment, please use (1,) or (n_env, 1) for the observation shape."
     ]
    }
   ],
   "source": [
    "env = StardewMineEnv(size=10)\n",
    "\n",
    "ppo_rewards, ppo_ores, ppo_floors, ppo_energy, ppo_heatmaps = evaluate_agent(\n",
    "    model, env, episodes=20, is_ppo=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a934ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame({\n",
    "    \"agent\": [\"PPO\"]*20,\n",
    "    \"reward\": ppo_rewards,\n",
    "    \"ore\": ppo_ores,\n",
    "    \"floor\": ppo_floors,\n",
    "    \"energy\": ppo_energy\n",
    "})\n",
    "\n",
    "results.to_csv(\"../results/stats.csv\", index=False)\n",
    "results.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a6aee6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_bar(\n",
    "    x=[\"PPO\"],\n",
    "    heights=[np.mean(ppo_rewards)],\n",
    "    xlabel=\"Algorithm\",\n",
    "    ylabel=\"Average Reward\",\n",
    "    title=\"PPO vs DQN Average Episode Reward\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7be6ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_heatmap(ppo_heatmaps[0], \"PPO Heatmap (Episode 1)\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hw2-py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
